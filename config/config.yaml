# Dataset Configuration
dataset:
  type: "standford_dogs"                   # Dataset type (mnist, cifar10, cifar100, gtrsb, flowers102, imagenet, standford_dogs)
  data_path: "data/StanfordDogsDataset.pth"             # Data path
  image_size: 64                  # Image size after resize
  channels: 3                     # Number of image channels (mnist: 1, cifar10: 3, ...)
  select_class_id: [0,1,2,3,4,5,6,7,8,9]  # Select class id for custom dataset
  num_classes: 10                 # Number of classes (mnist: 10, cifar10: 10, gtrsb: 43, flowers102: 102, imagenet: 1000, custom: ?)
#  class_imbalance_ratio: {}
  class_imbalance_ratio: {"0":0.1, "1":0.1, "2":0.1, "3":0.1,
                          "4":0.2, "5":0.2, "6":0.3, "7":0.1, "8":0.01, "9":0.05}       # Class imbalance ratio

  split_ratio:                    # Train/validation/test split ratios
    train: 0.8
    validation: 0.1
    test: 0.1

# GAN Model Configuration
model:
  name: "dcgan"                    # GAN model type ("dcgan", "wgan_gp", "cgan")
  save_path: "./experiments"      # Path to save the model
  load_pretrained: False          # Load pretrained model?

# Training Configuration
training:
  device: "cuda"                  # Device to use ("cuda" or "cpu")
  number_gpu: 1                   # Number of GPUs to use
  optimizer: "adam"               # Optimization algorithm (adam, rmsprop, sgd, adamw, nadam)
  batch_size: 64                  # Batch size
  noise_dimension: 100            # Size of noise vector
  latent_dim: 100                 # Size of latent dimension
  ngf: 64                         # Generator feature map size
  ndf: 64                         # Discriminator feature map size
  learning_rate: 0.0002           # Learning rate
  beta1: 0.5                      # Beta1 parameter for Adam optimizer
  num_epochs: 1                    # Number of epochs
  save_interval: 1                # Save model every n iterations
  early_stop: True               # Enable early stopping?
  patience: 10                     # Patience for early stopping
  save_loss: True                 # Save loss history?
  save_model_per_epoch: False     # Save model per epoch?
  gen_images_per_epoch: True      # Generate images per epoch?
  log_interval: 10                # Log interval
  use_lecam: False                 # Use LeCam regularizer?
  lecam_ratio: 0.1              # LeCam regularizer ratio
  init_ema: 100                   # Init EMA value when start
  decay_ema: 0.9                  # EMA decay rate
  start_epoch_ema: 100            # Start EMA after n epochs
  n_critic: 5                     # Number of critic updates per generator update (only for WGAN)
  lambda_gp: 1                    # Gradient penalty coefficient (only for WGAN)