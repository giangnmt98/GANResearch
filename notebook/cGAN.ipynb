{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cFx9M6QpQN-y"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "import numpy as np\n",
        "from scipy.linalg import sqrtm\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Define the Generator model\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, nz, nc, ngf, num_classes):\n",
        "      super(Generator, self).__init__()\n",
        "      self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "      self.main = nn.Sequential(\n",
        "          nn.ConvTranspose2d(nz + num_classes, ngf * 8, 4, 1, 0, bias=False),\n",
        "          nn.BatchNorm2d(ngf * 8),\n",
        "          nn.ReLU(True),\n",
        "          nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "          nn.BatchNorm2d(ngf * 4),\n",
        "          nn.ReLU(True),\n",
        "          nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "          nn.BatchNorm2d(ngf * 2),\n",
        "          nn.ReLU(True),\n",
        "          nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "          nn.BatchNorm2d(ngf),\n",
        "          nn.ReLU(True),\n",
        "          nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "          nn.Tanh()\n",
        "      )\n",
        "\n",
        "  def forward(self, noise, labels):\n",
        "      label_embedding = self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
        "      input = torch.cat((noise, label_embedding), 1)\n",
        "      return self.main(input)\n",
        "\n",
        "# Define the Discriminator model\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, nc, ndf, num_classes):\n",
        "      super(Discriminator, self).__init__()\n",
        "      self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "      self.main = nn.Sequential(\n",
        "          nn.Conv2d(nc + num_classes, ndf, 4, 2, 1, bias=False),\n",
        "          nn.LeakyReLU(0.2, inplace=True),\n",
        "          nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "          nn.BatchNorm2d(ndf * 2),\n",
        "          nn.LeakyReLU(0.2, inplace=True),\n",
        "          nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "          nn.BatchNorm2d(ndf * 4),\n",
        "          nn.LeakyReLU(0.2, inplace=True),\n",
        "          nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "          nn.BatchNorm2d(ndf * 8),\n",
        "          nn.LeakyReLU(0.2, inplace=True),\n",
        "          nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "          nn.Sigmoid()\n",
        "      )\n",
        "\n",
        "  def forward(self, img, labels):\n",
        "      label_embedding = self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
        "      label_embedding = label_embedding.expand(-1, -1, img.size(2), img.size(3))\n",
        "      input = torch.cat((img, label_embedding), 1)\n",
        "      return self.main(input)\n",
        "\n",
        "# Function to train the cGAN\n",
        "def train_cgan(generator, discriminator, dataloader, num_epochs=25, lr=0.0002, nz=100, num_classes=10):\n",
        "  criterion = nn.BCELoss()\n",
        "  optimizerD = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "  optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      for i, (real_images, labels) in enumerate(dataloader):\n",
        "          batch_size = real_images.size(0)\n",
        "          real_images = real_images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # Train Discriminator\n",
        "          discriminator.zero_grad()\n",
        "          real_labels = torch.full((batch_size,), 1, dtype=torch.float, device=device)\n",
        "          fake_labels = torch.full((batch_size,), 0, dtype=torch.float, device=device)\n",
        "\n",
        "          output = discriminator(real_images, labels).view(-1)\n",
        "          lossD_real = criterion(output, real_labels)\n",
        "          lossD_real.backward()\n",
        "\n",
        "          noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "          fake_images = generator(noise, labels)\n",
        "          output = discriminator(fake_images.detach(), labels).view(-1)\n",
        "          lossD_fake = criterion(output, fake_labels)\n",
        "          lossD_fake.backward()\n",
        "          optimizerD.step()\n",
        "\n",
        "          # Train Generator\n",
        "          generator.zero_grad()\n",
        "          output = discriminator(fake_images, labels).view(-1)\n",
        "          lossG = criterion(output, real_labels)\n",
        "          lossG.backward()\n",
        "          optimizerG.step()\n",
        "\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}] Loss D: {lossD_real + lossD_fake}, Loss G: {lossG}')\n",
        "\n",
        "# Create datasets\n",
        "def create_datasets(imbalance_ratios, batch_size=64):\n",
        "  train_transform = transforms.Compose([\n",
        "      transforms.Resize(64),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "  ])\n",
        "\n",
        "  fid_transform = transforms.Compose([\n",
        "      transforms.Resize(299),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "  ])\n",
        "\n",
        "  cifar10_train = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_transform)\n",
        "  cifar10_test = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=fid_transform)\n",
        "\n",
        "  train_size = int(0.8 * len(cifar10_train))\n",
        "  val_size = len(cifar10_train) - train_size\n",
        "  train_dataset, val_dataset = random_split(cifar10_train, [train_size, val_size])\n",
        "\n",
        "  targets = np.array([cifar10_train.targets[i] for i in train_dataset.indices])\n",
        "  indices = [i for class_id, ratio in imbalance_ratios.items()\n",
        "             for i in np.where(targets == class_id)[0][:int(len(np.where(targets == class_id)[0]) * ratio)]]\n",
        "\n",
        "  imbalanced_dataset = Subset(train_dataset, indices)\n",
        "\n",
        "  return (\n",
        "      DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True),\n",
        "      DataLoader(imbalanced_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True),\n",
        "      DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True),\n",
        "      DataLoader(cifar10_test, batch_size=batch_size, shuffle=False, num_workers=4, pin_memory=True)\n",
        "  )\n",
        "\n",
        "# Calculate FID score between two sets of features\n",
        "def calculate_fid(real_features, fake_features):\n",
        "  mu_real, sigma_real = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
        "  mu_fake, sigma_fake = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
        "\n",
        "  covmean = sqrtm(sigma_real @ sigma_fake)\n",
        "  if np.iscomplexobj(covmean):\n",
        "      covmean = covmean.real\n",
        "\n",
        "  fid = np.sum((mu_real - mu_fake) ** 2) + np.trace(sigma_real + sigma_fake - 2 * covmean)\n",
        "  return fid\n",
        "\n",
        "# Extract features class-by-class\n",
        "def extract_class_features(loader, model, class_id, generator=None):\n",
        "  model.eval()\n",
        "  features = []\n",
        "  with torch.no_grad():\n",
        "      for inputs, labels in loader:\n",
        "          mask = labels == class_id  # Filter inputs by class\n",
        "          if mask.sum() == 0:\n",
        "              continue\n",
        "          inputs = inputs[mask].to(device)\n",
        "\n",
        "          if generator:\n",
        "              # Generate fake images using the generator\n",
        "              noise = torch.randn(inputs.size(0), 100, 1, 1, device=device)\n",
        "              inputs = generator(noise, labels[mask])\n",
        "\n",
        "          # Resize inputs to 299x299 for Inception-v3\n",
        "          inputs = F.interpolate(inputs, size=(299, 299), mode='bilinear', align_corners=False)\n",
        "          outputs = model(inputs)  # Extract features\n",
        "          features.append(outputs.cpu().numpy())\n",
        "\n",
        "  return np.concatenate(features, axis=0)\n",
        "\n",
        "# Compare FID scores per class\n",
        "def compare_fid_scores(test_loader, generator_balanced, generator_imbalanced):\n",
        "  # Initialize the Inception-v3 model for feature extraction\n",
        "  inception = models.inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "  inception.fc = torch.nn.Identity()  # Replace the FC layer\n",
        "\n",
        "  fid_data = []\n",
        "\n",
        "  for class_id in range(10):  # Assuming CIFAR-10 has 10 classes\n",
        "      print(f\"Processing class {class_id}...\")\n",
        "\n",
        "      # Extract real features for the current class\n",
        "      real_features = extract_class_features(test_loader, inception, class_id)\n",
        "\n",
        "      # Extract fake features from the balanced and imbalanced generators\n",
        "      fake_features_balanced = extract_class_features(test_loader, inception, class_id, generator_balanced)\n",
        "      fake_features_imbalanced = extract_class_features(test_loader, inception, class_id, generator_imbalanced)\n",
        "\n",
        "      # Calculate FID scores\n",
        "      fid_balanced = calculate_fid(real_features, fake_features_balanced)\n",
        "      fid_imbalanced = calculate_fid(real_features, fake_features_imbalanced)\n",
        "\n",
        "      # Store results for this class\n",
        "      fid_data.append({\n",
        "          \"Class\": class_id,\n",
        "          \"FID (Balanced)\": fid_balanced,\n",
        "          \"FID (Imbalanced)\": fid_imbalanced,\n",
        "          \"Delta FID\": fid_imbalanced - fid_balanced\n",
        "      })\n",
        "\n",
        "  # Display the results\n",
        "  import pandas as pd\n",
        "  df = pd.DataFrame(fid_data)\n",
        "  print(df)\n",
        "\n",
        "  return df"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Hyperparameters\n",
        "nz = 100  # Size of z latent vector (i.e. size of generator input)\n",
        "nc = 3    # Number of channels in the training images. For color images this is 3\n",
        "ngf = 64  # Size of feature maps in generator\n",
        "ndf = 64  # Size of feature maps in discriminator\n",
        "num_classes = 10\n",
        "num_epochs = 25\n",
        "lr = 0.0002\n",
        "\n",
        "# Imbalance ratios for each class\n",
        "imbalance_ratios = {0: 0.01, 1: 0.01, 2: 0.02, 3: 0.05, 4: 0.4, 5: 0.5, 6: 0.6, 7: 0.7, 8: 0.8, 9: 0.9}\n",
        "# Create datasets\n",
        "train_loader_balanced, train_loader_imbalanced, val_loader, test_loader = create_datasets(imbalance_ratios)\n",
        "\n",
        "# Initialize models\n",
        "generator_balanced = Generator(nz, nc, ngf, num_classes).to(device)\n",
        "discriminator_balanced = Discriminator(nc, ndf, num_classes).to(device)\n",
        "\n",
        "generator_imbalanced = Generator(nz, nc, ngf, num_classes).to(device)\n",
        "discriminator_imbalanced = Discriminator(nc, ndf, num_classes).to(device)\n",
        "\n",
        "# Train models\n",
        "print(\"Training balanced cGAN...\")\n",
        "train_cgan(generator_balanced, discriminator_balanced, train_loader_balanced, num_epochs, lr, nz, num_classes)\n",
        "\n",
        "print(\"Training imbalanced cGAN...\")\n",
        "train_cgan(generator_imbalanced, discriminator_imbalanced, train_loader_imbalanced, num_epochs, lr, nz, num_classes)\n",
        "\n",
        "# Compare FID scores\n",
        "print(\"Comparing FID scores...\")\n",
        "compare_fid_scores(test_loader, generator_balanced, generator_imbalanced)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 474
        },
        "id": "9aBlNIiPR8gJ",
        "outputId": "54fa935c-8ae1-4463-e287-b7b195bdc9b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170M/170M [00:05<00:00, 30.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training balanced cGAN...\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-e67358706926>\u001b[0m in \u001b[0;36m<cell line: 24>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m# Train models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training balanced cGAN...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mtrain_cgan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training imbalanced cGAN...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-5cc20cb84034>\u001b[0m in \u001b[0;36mtrain_cgan\u001b[0;34m(generator, discriminator, dataloader, num_epochs, lr, nz, num_classes)\u001b[0m\n\u001b[1;32m     76\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m           \u001b[0mreal_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m           \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader, random_split, Subset\n",
        "import numpy as np\n",
        "from scipy.linalg import sqrtm\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Define the Generator model\n",
        "class Generator(nn.Module):\n",
        "  def __init__(self, nz, nc, ngf, num_classes):\n",
        "      super(Generator, self).__init__()\n",
        "      self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "      self.main = nn.Sequential(\n",
        "          nn.ConvTranspose2d(nz + num_classes, ngf * 8, 4, 1, 0, bias=False),\n",
        "          nn.BatchNorm2d(ngf * 8),\n",
        "          nn.ReLU(True),\n",
        "          nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "          nn.BatchNorm2d(ngf * 4),\n",
        "          nn.ReLU(True),\n",
        "          nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "          nn.BatchNorm2d(ngf * 2),\n",
        "          nn.ReLU(True),\n",
        "          nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "          nn.BatchNorm2d(ngf),\n",
        "          nn.ReLU(True),\n",
        "          nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "          nn.Tanh()\n",
        "      )\n",
        "\n",
        "  def forward(self, noise, labels):\n",
        "      label_embedding = self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
        "      input = torch.cat((noise, label_embedding), 1)\n",
        "      return self.main(input)\n",
        "\n",
        "# Define the Discriminator model\n",
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, nc, ndf, num_classes):\n",
        "      super(Discriminator, self).__init__()\n",
        "      self.label_emb = nn.Embedding(num_classes, num_classes)\n",
        "      self.main = nn.Sequential(\n",
        "          nn.Conv2d(nc + num_classes, ndf, 4, 2, 1, bias=False),\n",
        "          nn.LeakyReLU(0.2, inplace=True),\n",
        "          nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "          nn.BatchNorm2d(ndf * 2),\n",
        "          nn.LeakyReLU(0.2, inplace=True),\n",
        "          nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "          nn.BatchNorm2d(ndf * 4),\n",
        "          nn.LeakyReLU(0.2, inplace=True),\n",
        "          nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
        "          nn.BatchNorm2d(ndf * 8),\n",
        "          nn.LeakyReLU(0.2, inplace=True),\n",
        "          nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
        "          nn.Sigmoid()\n",
        "      )\n",
        "\n",
        "  def forward(self, img, labels):\n",
        "      label_embedding = self.label_emb(labels).unsqueeze(2).unsqueeze(3)\n",
        "      label_embedding = label_embedding.expand(-1, -1, img.size(2), img.size(3))\n",
        "      input = torch.cat((img, label_embedding), 1)\n",
        "      return self.main(input)\n",
        "\n",
        "# Function to train the cGAN\n",
        "def train_cgan(generator, discriminator, dataloader, num_epochs=25, lr=0.0002, nz=100, num_classes=10):\n",
        "  criterion = nn.BCELoss()\n",
        "  optimizerD = optim.Adam(discriminator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "  optimizerG = optim.Adam(generator.parameters(), lr=lr, betas=(0.5, 0.999))\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "      for i, (real_images, labels) in enumerate(dataloader):\n",
        "          batch_size = real_images.size(0)\n",
        "          real_images = real_images.to(device)\n",
        "          labels = labels.to(device)\n",
        "\n",
        "          # Train Discriminator\n",
        "          discriminator.zero_grad()\n",
        "          real_labels = torch.full((batch_size,), 1, dtype=torch.float, device=device)\n",
        "          fake_labels = torch.full((batch_size,), 0, dtype=torch.float, device=device)\n",
        "\n",
        "          output = discriminator(real_images, labels).view(-1)\n",
        "          lossD_real = criterion(output, real_labels)\n",
        "          lossD_real.backward()\n",
        "\n",
        "          noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "          fake_images = generator(noise, labels)\n",
        "          output = discriminator(fake_images.detach(), labels).view(-1)\n",
        "          lossD_fake = criterion(output, fake_labels)\n",
        "          lossD_fake.backward()\n",
        "          optimizerD.step()\n",
        "\n",
        "          # Train Generator\n",
        "          generator.zero_grad()\n",
        "          output = discriminator(fake_images, labels).view(-1)\n",
        "          lossG = criterion(output, real_labels)\n",
        "          lossG.backward()\n",
        "          optimizerG.step()\n",
        "\n",
        "      print(f'Epoch [{epoch+1}/{num_epochs}] Loss D: {lossD_real + lossD_fake}, Loss G: {lossG}')\n",
        "\n",
        "# Create datasets\n",
        "def create_datasets(imbalance_ratios, batch_size=64):\n",
        "  train_transform = transforms.Compose([\n",
        "      transforms.Resize(64),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "  ])\n",
        "\n",
        "  fid_transform = transforms.Compose([\n",
        "      transforms.Resize(299),\n",
        "      transforms.ToTensor(),\n",
        "      transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
        "  ])\n",
        "\n",
        "  cifar10_train = datasets.CIFAR10(root=\"./data\", train=True, download=True, transform=train_transform)\n",
        "  cifar10_test = datasets.CIFAR10(root=\"./data\", train=False, download=True, transform=fid_transform)\n",
        "\n",
        "  train_size = int(0.8 * len(cifar10_train))\n",
        "  val_size = len(cifar10_train) - train_size\n",
        "  train_dataset, val_dataset = random_split(cifar10_train, [train_size, val_size])\n",
        "\n",
        "  targets = np.array([cifar10_train.targets[i] for i in train_dataset.indices])\n",
        "  indices = [i for class_id, ratio in imbalance_ratios.items()\n",
        "             for i in np.where(targets == class_id)[0][:int(len(np.where(targets == class_id)[0]) * ratio)]]\n",
        "\n",
        "  imbalanced_dataset = Subset(train_dataset, indices)\n",
        "\n",
        "  return (\n",
        "      DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True),\n",
        "      DataLoader(imbalanced_dataset, batch_size=batch_size, shuffle=True, num_workers=1, pin_memory=True),\n",
        "      DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True),\n",
        "      DataLoader(cifar10_test, batch_size=batch_size, shuffle=False, num_workers=1, pin_memory=True)\n",
        "  )\n",
        "\n",
        "# Calculate FID score between two sets of features\n",
        "def calculate_fid(real_features, fake_features):\n",
        "  mu_real, sigma_real = real_features.mean(axis=0), np.cov(real_features, rowvar=False)\n",
        "  mu_fake, sigma_fake = fake_features.mean(axis=0), np.cov(fake_features, rowvar=False)\n",
        "\n",
        "  covmean = sqrtm(sigma_real @ sigma_fake)\n",
        "  if np.iscomplexobj(covmean):\n",
        "      covmean = covmean.real\n",
        "\n",
        "  fid = np.sum((mu_real - mu_fake) ** 2) + np.trace(sigma_real + sigma_fake - 2 * covmean)\n",
        "  return fid\n",
        "\n",
        "# Extract features class-by-class\n",
        "def extract_class_features(loader, model, class_id, generator=None):\n",
        "  model.eval()\n",
        "  features = []\n",
        "  with torch.no_grad():\n",
        "      for inputs, labels in loader:\n",
        "          mask = labels == class_id  # Filter inputs by class\n",
        "          if mask.sum() == 0:\n",
        "              continue\n",
        "          inputs = inputs[mask].to(device)\n",
        "\n",
        "          if generator:\n",
        "              # Generate fake images using the generator\n",
        "              noise = torch.randn(inputs.size(0), 100, 1, 1, device=device)\n",
        "              inputs = generator(noise, labels[mask])\n",
        "\n",
        "          # Resize inputs to 299x299 for Inception-v3\n",
        "          inputs = F.interpolate(inputs, size=(299, 299), mode='bilinear', align_corners=False)\n",
        "          outputs = model(inputs)  # Extract features\n",
        "          features.append(outputs.cpu().numpy())\n",
        "\n",
        "  return np.concatenate(features, axis=0)\n",
        "\n",
        "# Compare FID scores per class\n",
        "def compare_fid_scores(test_loader, generator_balanced, generator_imbalanced):\n",
        "  # Initialize the Inception-v3 model for feature extraction\n",
        "  inception = models.inception_v3(pretrained=True, transform_input=False).to(device)\n",
        "  inception.fc = torch.nn.Identity()  # Replace the FC layer\n",
        "\n",
        "  fid_data = []\n",
        "\n",
        "  for class_id in range(10):  # Assuming CIFAR-10 has 10 classes\n",
        "      print(f\"Processing class {class_id}...\")\n",
        "\n",
        "      # Extract real features for the current class\n",
        "      real_features = extract_class_features(test_loader, inception, class_id)\n",
        "\n",
        "      # Extract fake features from the balanced and imbalanced generators\n",
        "      fake_features_balanced = extract_class_features(test_loader, inception, class_id, generator_balanced)\n",
        "      fake_features_imbalanced = extract_class_features(test_loader, inception, class_id, generator_imbalanced)\n",
        "\n",
        "      # Calculate FID scores\n",
        "      fid_balanced = calculate_fid(real_features, fake_features_balanced)\n",
        "      fid_imbalanced = calculate_fid(real_features, fake_features_imbalanced)\n",
        "\n",
        "      # Store results for this class\n",
        "      fid_data.append({\n",
        "          \"Class\": class_id,\n",
        "          \"FID (Balanced)\": fid_balanced,\n",
        "          \"FID (Imbalanced)\": fid_imbalanced,\n",
        "          \"Delta FID\": fid_imbalanced - fid_balanced\n",
        "      })\n",
        "\n",
        "  # Display the results\n",
        "  import pandas as pd\n",
        "  df = pd.DataFrame(fid_data)\n",
        "  print(df)\n",
        "\n",
        "  return df\n",
        "\n",
        "# Main function to execute the training and evaluation\n",
        "def main():\n",
        "  # Hyperparameters\n",
        "  nz = 100  # Size of z latent vector (i.e. size of generator input)\n",
        "  nc = 3    # Number of channels in the training images. For color images this is 3\n",
        "  ngf = 64  # Size of feature maps in generator\n",
        "  ndf = 64  # Size of feature maps in discriminator\n",
        "  num_classes = 10\n",
        "  num_epochs = 25\n",
        "  lr = 0.0002\n",
        "\n",
        "  # Imbalance ratios for each class\n",
        "  imbalance_ratios = {0: 0.01, 1: 0.01, 2: 0.02, 3: 0.05, 4: 0.4, 5: 0.5, 6: 0.6, 7: 0.7, 8: 0.8, 9: 0.9}\n",
        "\n",
        "  # Create datasets\n",
        "  train_loader_balanced, train_loader_imbalanced, val_loader, test_loader = create_datasets(imbalance_ratios)\n",
        "\n",
        "  # Initialize models\n",
        "  generator_balanced = Generator(nz, nc, ngf, num_classes).to(device)\n",
        "  discriminator_balanced = Discriminator(nc, ndf, num_classes).to(device)\n",
        "\n",
        "  generator_imbalanced = Generator(nz, nc, ngf, num_classes).to(device)\n",
        "  discriminator_imbalanced = Discriminator(nc, ndf, num_classes).to(device)\n",
        "\n",
        "  # Train models\n",
        "  print(\"Training balanced cGAN...\")\n",
        "  train_cgan(generator_balanced, discriminator_balanced, train_loader_balanced, num_epochs, lr, nz, num_classes)\n",
        "\n",
        "  print(\"Training imbalanced cGAN...\")\n",
        "  train_cgan(generator_imbalanced, discriminator_imbalanced, train_loader_imbalanced, num_epochs, lr, nz, num_classes)\n",
        "\n",
        "  # Compare FID scores\n",
        "  print(\"Comparing FID scores...\")\n",
        "  compare_fid_scores(test_loader, generator_balanced, generator_imbalanced)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "  main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 584
        },
        "id": "URsUAtf_ShrK",
        "outputId": "e013744e-56db-4a17-b090-981cacd61431"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Files already downloaded and verified\n",
            "Files already downloaded and verified\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:617: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training balanced cGAN...\n",
            "Epoch [1/25] Loss D: 1.122218132019043, Loss G: 1.7036888599395752\n",
            "Epoch [2/25] Loss D: 0.6722404956817627, Loss G: 2.850545883178711\n",
            "Epoch [3/25] Loss D: 0.3328244686126709, Loss G: 2.921764850616455\n",
            "Epoch [4/25] Loss D: 0.43519553542137146, Loss G: 2.9261364936828613\n",
            "Epoch [5/25] Loss D: 0.3724260926246643, Loss G: 2.7494711875915527\n",
            "Epoch [6/25] Loss D: 0.03655776381492615, Loss G: 4.265316963195801\n",
            "Epoch [7/25] Loss D: 0.07403385639190674, Loss G: 4.79141902923584\n",
            "Epoch [8/25] Loss D: 0.0925154760479927, Loss G: 3.9526398181915283\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-7d2eb5bc5e54>\u001b[0m in \u001b[0;36m<cell line: 248>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"__main__\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 249\u001b[0;31m   \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-1-7d2eb5bc5e54>\u001b[0m in \u001b[0;36mmain\u001b[0;34m()\u001b[0m\n\u001b[1;32m    237\u001b[0m   \u001b[0;31m# Train models\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    238\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training balanced cGAN...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 239\u001b[0;31m   \u001b[0mtrain_cgan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgenerator_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdiscriminator_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader_balanced\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnz\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_classes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    241\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Training imbalanced cGAN...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-1-7d2eb5bc5e54>\u001b[0m in \u001b[0;36mtrain_cgan\u001b[0;34m(generator, discriminator, dataloader, num_epochs, lr, nz, num_classes)\u001b[0m\n\u001b[1;32m     77\u001b[0m       \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mreal_images\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m           \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 79\u001b[0;31m           \u001b[0mreal_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreal_images\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     80\u001b[0m           \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torchvision.utils as vutils\n"
      ],
      "metadata": {
        "id": "x0x2rotmW4SX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(dataset_name='cifar10', dataroot='./data', image_size=64, batch_size=64):\n",
        "    if dataset_name == 'cifar10':\n",
        "        dataset = dset.CIFAR10(\n",
        "            root=dataroot, download=True,\n",
        "            transform=transforms.Compose([\n",
        "                transforms.Resize(image_size),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
        "            ])\n",
        "        )\n",
        "        nc = 3  # CIFAR-10 có 3 kênh (RGB)\n",
        "    dataloader = torch.utils.data.DataLoader(\n",
        "        dataset, batch_size=batch_size, shuffle=True, num_workers=1\n",
        "    )\n",
        "    return dataloader, nc"
      ],
      "metadata": {
        "id": "R3HoyykyW_uH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self, ngpu, nz, ngf, nc):\n",
        "        super(Generator, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 8),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 4),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf * 2),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ngf),\n",
        "            nn.ReLU(True),\n",
        "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
        "            nn.Tanh()\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input)\n"
      ],
      "metadata": {
        "id": "HsDCdVVtXA2W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Critic(nn.Module):\n",
        "    def __init__(self, ngpu, ndf, nc):\n",
        "        super(Critic, self).__init__()\n",
        "        self.main = nn.Sequential(\n",
        "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 2),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
        "            nn.BatchNorm2d(ndf * 4),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False)\n",
        "        )\n",
        "\n",
        "    def forward(self, input):\n",
        "        return self.main(input).view(-1)  # Output is a scalar score per input\n"
      ],
      "metadata": {
        "id": "Bj8X2O8RXCUW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def gradient_penalty(critic, real_data, fake_data, device):\n",
        "    batch_size = real_data.size(0)\n",
        "    epsilon = torch.rand(batch_size, 1, 1, 1, device=device)\n",
        "    interpolated = epsilon * real_data + (1 - epsilon) * fake_data\n",
        "    interpolated.requires_grad_(True)\n",
        "\n",
        "    score_interpolated = critic(interpolated)\n",
        "\n",
        "    grad_outputs = torch.ones_like(score_interpolated, device=device)\n",
        "    gradients = torch.autograd.grad(\n",
        "        outputs=score_interpolated,\n",
        "        inputs=interpolated,\n",
        "        grad_outputs=grad_outputs,\n",
        "        create_graph=True,\n",
        "        retain_graph=True,\n",
        "        only_inputs=True\n",
        "    )[0]\n",
        "\n",
        "    gradients = gradients.view(batch_size, -1)\n",
        "    gradient_norm = gradients.norm(2, dim=1)\n",
        "    penalty = ((gradient_norm - 1) ** 2).mean()\n",
        "    return penalty\n"
      ],
      "metadata": {
        "id": "Mw29Ptc2XEY3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nz = 100  # Latent vector size\n",
        "ngf = 64  # Generator feature map size\n",
        "ndf = 64  # Critic feature map size\n",
        "ngpu = 1  # Number of GPUs\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "netG = Generator(ngpu, nz, ngf, 3).to(device)\n",
        "netC = Critic(ngpu, ndf, 3).to(device)\n",
        "\n",
        "optimizerG = optim.Adam(netG.parameters(), lr=0.0001, betas=(0.0, 0.9))\n",
        "optimizerC = optim.Adam(netC.parameters(), lr=0.0001, betas=(0.0, 0.9))\n"
      ],
      "metadata": {
        "id": "rYNw0c0BXFxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n_critic = 5  # Update critic 5 times per generator update\n",
        "lambda_gp = 10  # Gradient penalty weight\n",
        "# Khởi tạo dataloader và số lượng kênh\n",
        "dataloader, nc = load_data('cifar10')\n",
        "\n",
        "for epoch in range(25):\n",
        "    for i, data in enumerate(dataloader):\n",
        "        # Train Critic\n",
        "        netC.zero_grad()\n",
        "        real_data = data[0].to(device)\n",
        "\n",
        "        batch_size = real_data.size(0)\n",
        "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "        fake_data = netG(noise).detach()\n",
        "\n",
        "        real_score = netC(real_data).mean()\n",
        "        fake_score = netC(fake_data).mean()\n",
        "\n",
        "        # Compute gradient penalty\n",
        "        gp = gradient_penalty(netC, real_data, fake_data, device)\n",
        "\n",
        "        # Critic loss\n",
        "        lossC = fake_score - real_score + lambda_gp * gp\n",
        "        lossC.backward()\n",
        "        optimizerC.step()\n",
        "\n",
        "        # Update Generator every n_critic iterations\n",
        "        if i % n_critic == 0:\n",
        "            netG.zero_grad()\n",
        "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
        "            fake_data = netG(noise)\n",
        "            lossG = -netC(fake_data).mean()\n",
        "            lossG.backward()\n",
        "            optimizerG.step()\n",
        "\n",
        "        if i % 100 == 0:\n",
        "            print(f'[{epoch}/{25}][{i}/{len(dataloader)}] '\n",
        "                  f'Loss_C: {lossC.item():.4f} Loss_G: {lossG.item():.4f}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "70nPMd5AXHD2",
        "outputId": "4edc5e0f-fb1e-4c5c-fd6c-ba1309206888"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files already downloaded and verified\n",
            "[0/25][0/782] Loss_C: 206.8642 Loss_G: 0.3810\n",
            "[0/25][100/782] Loss_C: -1.1295 Loss_G: 1.2748\n",
            "[0/25][200/782] Loss_C: -0.7499 Loss_G: 0.7439\n",
            "[0/25][300/782] Loss_C: -0.3188 Loss_G: 0.2283\n",
            "[0/25][400/782] Loss_C: -0.3124 Loss_G: 0.6594\n",
            "[0/25][500/782] Loss_C: -0.3546 Loss_G: 1.0025\n",
            "[0/25][600/782] Loss_C: -0.2581 Loss_G: 1.0145\n",
            "[0/25][700/782] Loss_C: -0.2458 Loss_G: 1.1796\n",
            "[1/25][0/782] Loss_C: -0.2467 Loss_G: 1.2689\n",
            "[1/25][100/782] Loss_C: -0.2634 Loss_G: 1.3794\n",
            "[1/25][200/782] Loss_C: -0.3040 Loss_G: 1.4249\n",
            "[1/25][300/782] Loss_C: -0.2925 Loss_G: 1.4357\n",
            "[1/25][400/782] Loss_C: -0.3044 Loss_G: 1.4063\n",
            "[1/25][500/782] Loss_C: -0.3014 Loss_G: 1.4268\n",
            "[1/25][600/782] Loss_C: -0.2624 Loss_G: 1.3171\n",
            "[1/25][700/782] Loss_C: 0.1467 Loss_G: 1.2153\n",
            "[2/25][0/782] Loss_C: -0.2333 Loss_G: 1.1716\n",
            "[2/25][100/782] Loss_C: -0.2600 Loss_G: 1.2413\n",
            "[2/25][200/782] Loss_C: -0.2287 Loss_G: 1.0703\n",
            "[2/25][300/782] Loss_C: -0.2103 Loss_G: 0.9555\n",
            "[2/25][400/782] Loss_C: -0.2198 Loss_G: 1.0827\n",
            "[2/25][500/782] Loss_C: -0.2391 Loss_G: 1.0436\n",
            "[2/25][600/782] Loss_C: -0.2597 Loss_G: 1.1281\n",
            "[2/25][700/782] Loss_C: -0.2354 Loss_G: 1.0757\n",
            "[3/25][0/782] Loss_C: -0.1526 Loss_G: 1.1436\n",
            "[3/25][100/782] Loss_C: -0.2206 Loss_G: 1.0352\n",
            "[3/25][200/782] Loss_C: -0.1903 Loss_G: 1.1369\n",
            "[3/25][300/782] Loss_C: -0.2472 Loss_G: 1.1029\n",
            "[3/25][400/782] Loss_C: -0.1701 Loss_G: 1.0286\n",
            "[3/25][500/782] Loss_C: -0.2305 Loss_G: 1.1701\n",
            "[3/25][600/782] Loss_C: -0.1905 Loss_G: 1.1134\n",
            "[3/25][700/782] Loss_C: -0.1768 Loss_G: 1.0480\n",
            "[4/25][0/782] Loss_C: -0.1279 Loss_G: 1.0981\n",
            "[4/25][100/782] Loss_C: -0.2072 Loss_G: 1.1455\n",
            "[4/25][200/782] Loss_C: -0.1878 Loss_G: 1.1578\n",
            "[4/25][300/782] Loss_C: -0.1618 Loss_G: 1.2964\n",
            "[4/25][400/782] Loss_C: -0.2093 Loss_G: 1.2210\n",
            "[4/25][500/782] Loss_C: -0.1990 Loss_G: 1.1961\n",
            "[4/25][600/782] Loss_C: -0.1597 Loss_G: 1.1535\n",
            "[4/25][700/782] Loss_C: -0.1602 Loss_G: 1.2285\n",
            "[5/25][0/782] Loss_C: -0.1624 Loss_G: 1.2202\n",
            "[5/25][100/782] Loss_C: -0.1718 Loss_G: 1.2371\n",
            "[5/25][200/782] Loss_C: -0.1338 Loss_G: 1.2376\n",
            "[5/25][300/782] Loss_C: -0.0745 Loss_G: 1.2050\n",
            "[5/25][400/782] Loss_C: -0.0807 Loss_G: 1.2587\n",
            "[5/25][500/782] Loss_C: -0.1241 Loss_G: 1.1970\n",
            "[5/25][600/782] Loss_C: -0.1627 Loss_G: 1.2617\n",
            "[5/25][700/782] Loss_C: -0.1109 Loss_G: 1.3120\n",
            "[6/25][0/782] Loss_C: -0.1159 Loss_G: 1.2359\n",
            "[6/25][100/782] Loss_C: -0.1293 Loss_G: 1.2669\n",
            "[6/25][200/782] Loss_C: -0.1375 Loss_G: 1.2380\n",
            "[6/25][300/782] Loss_C: -0.1319 Loss_G: 1.2434\n",
            "[6/25][400/782] Loss_C: -0.0808 Loss_G: 1.2699\n",
            "[6/25][500/782] Loss_C: -0.1545 Loss_G: 1.2774\n",
            "[6/25][600/782] Loss_C: -0.1505 Loss_G: 1.2110\n",
            "[6/25][700/782] Loss_C: -0.1320 Loss_G: 1.2837\n",
            "[7/25][0/782] Loss_C: -0.0089 Loss_G: 1.1513\n",
            "[7/25][100/782] Loss_C: -0.1427 Loss_G: 1.1958\n",
            "[7/25][200/782] Loss_C: -0.0977 Loss_G: 1.1998\n",
            "[7/25][300/782] Loss_C: -0.1314 Loss_G: 1.2353\n",
            "[7/25][400/782] Loss_C: -0.1388 Loss_G: 1.1971\n",
            "[7/25][500/782] Loss_C: -0.0792 Loss_G: 1.2225\n",
            "[7/25][600/782] Loss_C: -0.1362 Loss_G: 1.2312\n",
            "[7/25][700/782] Loss_C: -0.0344 Loss_G: 1.2434\n",
            "[8/25][0/782] Loss_C: 0.0070 Loss_G: 1.2567\n",
            "[8/25][100/782] Loss_C: -0.0818 Loss_G: 1.2744\n",
            "[8/25][200/782] Loss_C: -0.1509 Loss_G: 1.2274\n",
            "[8/25][300/782] Loss_C: -0.1353 Loss_G: 1.2793\n",
            "[8/25][400/782] Loss_C: -0.1583 Loss_G: 1.2503\n",
            "[8/25][500/782] Loss_C: -0.1179 Loss_G: 1.2608\n",
            "[8/25][600/782] Loss_C: -0.0714 Loss_G: 1.2724\n",
            "[8/25][700/782] Loss_C: -0.0160 Loss_G: 1.3423\n",
            "[9/25][0/782] Loss_C: -0.1004 Loss_G: 1.2714\n",
            "[9/25][100/782] Loss_C: -0.0695 Loss_G: 1.3833\n",
            "[9/25][200/782] Loss_C: -0.0823 Loss_G: 1.3016\n",
            "[9/25][300/782] Loss_C: -0.1275 Loss_G: 1.3765\n",
            "[9/25][400/782] Loss_C: -0.0641 Loss_G: 1.3377\n",
            "[9/25][500/782] Loss_C: -0.1093 Loss_G: 1.3852\n",
            "[9/25][600/782] Loss_C: -0.1502 Loss_G: 1.3607\n",
            "[9/25][700/782] Loss_C: -0.0902 Loss_G: 1.3747\n",
            "[10/25][0/782] Loss_C: -0.0502 Loss_G: 1.3352\n",
            "[10/25][100/782] Loss_C: -0.0996 Loss_G: 1.4348\n",
            "[10/25][200/782] Loss_C: -0.1021 Loss_G: 1.3866\n",
            "[10/25][300/782] Loss_C: -0.0823 Loss_G: 1.4200\n",
            "[10/25][400/782] Loss_C: -0.1196 Loss_G: 1.4262\n",
            "[10/25][500/782] Loss_C: -0.1209 Loss_G: 1.4537\n",
            "[10/25][600/782] Loss_C: -0.1300 Loss_G: 1.4623\n",
            "[10/25][700/782] Loss_C: -0.0416 Loss_G: 1.3914\n",
            "[11/25][0/782] Loss_C: -0.0738 Loss_G: 1.4376\n",
            "[11/25][100/782] Loss_C: -0.1095 Loss_G: 1.4316\n",
            "[11/25][200/782] Loss_C: -0.1435 Loss_G: 1.3921\n",
            "[11/25][300/782] Loss_C: -0.1091 Loss_G: 1.4880\n",
            "[11/25][400/782] Loss_C: -0.1850 Loss_G: 1.4580\n",
            "[11/25][500/782] Loss_C: -0.0944 Loss_G: 1.5255\n",
            "[11/25][600/782] Loss_C: -0.1145 Loss_G: 1.4860\n",
            "[11/25][700/782] Loss_C: -0.1189 Loss_G: 1.4650\n",
            "[12/25][0/782] Loss_C: -0.0855 Loss_G: 1.3935\n",
            "[12/25][100/782] Loss_C: -0.0929 Loss_G: 1.5031\n",
            "[12/25][200/782] Loss_C: -0.1076 Loss_G: 1.4405\n",
            "[12/25][300/782] Loss_C: -0.1375 Loss_G: 1.4635\n",
            "[12/25][400/782] Loss_C: -0.1028 Loss_G: 1.4946\n",
            "[12/25][500/782] Loss_C: -0.0755 Loss_G: 1.4394\n",
            "[12/25][600/782] Loss_C: -0.1076 Loss_G: 1.4849\n",
            "[12/25][700/782] Loss_C: -0.0989 Loss_G: 1.5073\n",
            "[13/25][0/782] Loss_C: -0.0206 Loss_G: 1.4525\n",
            "[13/25][100/782] Loss_C: -0.0854 Loss_G: 1.5660\n",
            "[13/25][200/782] Loss_C: -0.0942 Loss_G: 1.4761\n",
            "[13/25][300/782] Loss_C: -0.0905 Loss_G: 1.4869\n",
            "[13/25][400/782] Loss_C: -0.0520 Loss_G: 1.4601\n",
            "[13/25][500/782] Loss_C: -0.0823 Loss_G: 1.4613\n",
            "[13/25][600/782] Loss_C: -0.0983 Loss_G: 1.4394\n",
            "[13/25][700/782] Loss_C: -0.0764 Loss_G: 1.3924\n",
            "[14/25][0/782] Loss_C: -0.0890 Loss_G: 1.4874\n",
            "[14/25][100/782] Loss_C: -0.0940 Loss_G: 1.3796\n",
            "[14/25][200/782] Loss_C: -0.0945 Loss_G: 1.4477\n",
            "[14/25][300/782] Loss_C: -0.0261 Loss_G: 1.3909\n",
            "[14/25][400/782] Loss_C: -0.0878 Loss_G: 1.4204\n",
            "[14/25][500/782] Loss_C: -0.0611 Loss_G: 1.3959\n",
            "[14/25][600/782] Loss_C: -0.0796 Loss_G: 1.4083\n",
            "[14/25][700/782] Loss_C: -0.0745 Loss_G: 1.4216\n",
            "[15/25][0/782] Loss_C: -0.0604 Loss_G: 1.3509\n",
            "[15/25][100/782] Loss_C: -0.0712 Loss_G: 1.3420\n",
            "[15/25][200/782] Loss_C: -0.0222 Loss_G: 1.3952\n",
            "[15/25][300/782] Loss_C: -0.0902 Loss_G: 1.3950\n",
            "[15/25][400/782] Loss_C: -0.0373 Loss_G: 1.3323\n",
            "[15/25][500/782] Loss_C: -0.1062 Loss_G: 1.3152\n",
            "[15/25][600/782] Loss_C: -0.0999 Loss_G: 1.4300\n",
            "[15/25][700/782] Loss_C: -0.0672 Loss_G: 1.3894\n",
            "[16/25][0/782] Loss_C: -0.0334 Loss_G: 1.3161\n",
            "[16/25][100/782] Loss_C: -0.0683 Loss_G: 1.4530\n",
            "[16/25][200/782] Loss_C: -0.0925 Loss_G: 1.3503\n",
            "[16/25][300/782] Loss_C: -0.0668 Loss_G: 1.3091\n",
            "[16/25][400/782] Loss_C: -0.0375 Loss_G: 1.3414\n",
            "[16/25][500/782] Loss_C: -0.0598 Loss_G: 1.2692\n",
            "[16/25][600/782] Loss_C: -0.1016 Loss_G: 1.3231\n",
            "[16/25][700/782] Loss_C: -0.0733 Loss_G: 1.3190\n",
            "[17/25][0/782] Loss_C: 0.0492 Loss_G: 1.3057\n",
            "[17/25][100/782] Loss_C: -0.0720 Loss_G: 1.2884\n",
            "[17/25][200/782] Loss_C: -0.0629 Loss_G: 1.2870\n",
            "[17/25][300/782] Loss_C: -0.0521 Loss_G: 1.3158\n",
            "[17/25][400/782] Loss_C: -0.0451 Loss_G: 1.2714\n",
            "[17/25][500/782] Loss_C: -0.0806 Loss_G: 1.2441\n",
            "[17/25][600/782] Loss_C: -0.0844 Loss_G: 1.3316\n",
            "[17/25][700/782] Loss_C: -0.0244 Loss_G: 1.3500\n",
            "[18/25][0/782] Loss_C: -0.0441 Loss_G: 1.2433\n",
            "[18/25][100/782] Loss_C: -0.0630 Loss_G: 1.2420\n",
            "[18/25][200/782] Loss_C: -0.0503 Loss_G: 1.2510\n",
            "[18/25][300/782] Loss_C: -0.0737 Loss_G: 1.2925\n",
            "[18/25][400/782] Loss_C: -0.0843 Loss_G: 1.2842\n",
            "[18/25][500/782] Loss_C: -0.0987 Loss_G: 1.2540\n",
            "[18/25][600/782] Loss_C: -0.0662 Loss_G: 1.3325\n",
            "[18/25][700/782] Loss_C: -0.0621 Loss_G: 1.3104\n",
            "[19/25][0/782] Loss_C: -0.0356 Loss_G: 1.3390\n",
            "[19/25][100/782] Loss_C: -0.0521 Loss_G: 1.3421\n",
            "[19/25][200/782] Loss_C: -0.0745 Loss_G: 1.3788\n",
            "[19/25][300/782] Loss_C: -0.0243 Loss_G: 1.3186\n",
            "[19/25][400/782] Loss_C: -0.0397 Loss_G: 1.2653\n",
            "[19/25][500/782] Loss_C: -0.0287 Loss_G: 1.3220\n",
            "[19/25][600/782] Loss_C: -0.0593 Loss_G: 1.2971\n",
            "[19/25][700/782] Loss_C: -0.0686 Loss_G: 1.4023\n",
            "[20/25][0/782] Loss_C: -0.0215 Loss_G: 1.2654\n",
            "[20/25][100/782] Loss_C: -0.0514 Loss_G: 1.2893\n",
            "[20/25][200/782] Loss_C: 0.0029 Loss_G: 1.3578\n",
            "[20/25][300/782] Loss_C: 0.0125 Loss_G: 1.2909\n",
            "[20/25][400/782] Loss_C: -0.0955 Loss_G: 1.3712\n",
            "[20/25][500/782] Loss_C: -0.0842 Loss_G: 1.3102\n",
            "[20/25][600/782] Loss_C: -0.1012 Loss_G: 1.2472\n",
            "[20/25][700/782] Loss_C: -0.0620 Loss_G: 1.3914\n",
            "[21/25][0/782] Loss_C: -0.0552 Loss_G: 1.3105\n",
            "[21/25][100/782] Loss_C: -0.0606 Loss_G: 1.2769\n",
            "[21/25][200/782] Loss_C: -0.0632 Loss_G: 1.2790\n",
            "[21/25][300/782] Loss_C: -0.0472 Loss_G: 1.3051\n",
            "[21/25][400/782] Loss_C: -0.0692 Loss_G: 1.3229\n",
            "[21/25][500/782] Loss_C: -0.0537 Loss_G: 1.3433\n",
            "[21/25][600/782] Loss_C: -0.0691 Loss_G: 1.3381\n",
            "[21/25][700/782] Loss_C: -0.0116 Loss_G: 1.2774\n",
            "[22/25][0/782] Loss_C: -0.0595 Loss_G: 1.2760\n",
            "[22/25][100/782] Loss_C: -0.0445 Loss_G: 1.2318\n",
            "[22/25][200/782] Loss_C: -0.0606 Loss_G: 1.2852\n",
            "[22/25][300/782] Loss_C: -0.0348 Loss_G: 1.2205\n",
            "[22/25][400/782] Loss_C: -0.0909 Loss_G: 1.2797\n",
            "[22/25][500/782] Loss_C: -0.0422 Loss_G: 1.2434\n",
            "[22/25][600/782] Loss_C: -0.0406 Loss_G: 1.2868\n",
            "[22/25][700/782] Loss_C: -0.0565 Loss_G: 1.2928\n",
            "[23/25][0/782] Loss_C: -0.0409 Loss_G: 1.2368\n",
            "[23/25][100/782] Loss_C: -0.0519 Loss_G: 1.3101\n",
            "[23/25][200/782] Loss_C: -0.0391 Loss_G: 1.2607\n",
            "[23/25][300/782] Loss_C: -0.0362 Loss_G: 1.3085\n",
            "[23/25][400/782] Loss_C: -0.0465 Loss_G: 1.2270\n",
            "[23/25][500/782] Loss_C: -0.0124 Loss_G: 1.2566\n",
            "[23/25][600/782] Loss_C: -0.0797 Loss_G: 1.2611\n",
            "[23/25][700/782] Loss_C: -0.0499 Loss_G: 1.2607\n",
            "[24/25][0/782] Loss_C: -0.0115 Loss_G: 1.2346\n",
            "[24/25][100/782] Loss_C: 0.0908 Loss_G: 1.1404\n",
            "[24/25][200/782] Loss_C: -0.1498 Loss_G: 1.2539\n",
            "[24/25][300/782] Loss_C: -0.0677 Loss_G: 1.3476\n",
            "[24/25][400/782] Loss_C: -0.0726 Loss_G: 1.2171\n",
            "[24/25][500/782] Loss_C: -0.0879 Loss_G: 1.2210\n",
            "[24/25][600/782] Loss_C: -0.0195 Loss_G: 1.2660\n",
            "[24/25][700/782] Loss_C: -0.0600 Loss_G: 1.3230\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PLRe4aq3XJCm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}