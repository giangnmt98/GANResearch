{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "x0x2rotmW4SX"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision.datasets as dset\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.utils as vutils\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "R3HoyykyW_uH"
   },
   "outputs": [],
   "source": [
    "def load_data(dataset_name='cifar10', dataroot='./data', image_size=64, batch_size=64):\n",
    "    if dataset_name == 'cifar10':\n",
    "        dataset = dset.CIFAR10(\n",
    "            root=dataroot, download=True,\n",
    "            transform=transforms.Compose([\n",
    "                transforms.Resize(image_size),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n",
    "            ])\n",
    "        )\n",
    "        nc = 3  # CIFAR-10 có 3 kênh (RGB)\n",
    "    dataloader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=batch_size, shuffle=True, num_workers=1\n",
    "    )\n",
    "    return dataloader, nc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HsDCdVVtXA2W"
   },
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, nz, ngf, nc):\n",
    "        super(Generator, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.ConvTranspose2d(nz, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Bj8X2O8RXCUW"
   },
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    def __init__(self, ndf, nc):\n",
    "        super(Critic, self).__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, 1, 4, 1, 0, bias=False)\n",
    "        )\n",
    "\n",
    "    def forward(self, input):\n",
    "        return self.main(input).view(-1)  # Output is a scalar score per input\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "Mw29Ptc2XEY3"
   },
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, real_data, fake_data, device):\n",
    "    batch_size = real_data.size(0)\n",
    "    epsilon = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "    interpolated = epsilon * real_data + (1 - epsilon) * fake_data\n",
    "    interpolated.requires_grad_(True)\n",
    "\n",
    "    score_interpolated = critic(interpolated)\n",
    "\n",
    "    grad_outputs = torch.ones_like(score_interpolated, device=device)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=score_interpolated,\n",
    "        inputs=interpolated,\n",
    "        grad_outputs=grad_outputs,\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "        only_inputs=True\n",
    "    )[0]\n",
    "\n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    return penalty\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "rYNw0c0BXFxW"
   },
   "outputs": [],
   "source": [
    "nz = 100  # Latent vector size\n",
    "ngf = 64  # Generator feature map size\n",
    "ndf = 64  # Critic feature map size\n",
    "ngpu = 1  # Number of GPUs\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "netG = Generator(ngpu, nz, ngf, 3).to(device)\n",
    "netC = Critic(ngpu, ndf, 3).to(device)\n",
    "\n",
    "optimizerG = optim.Adam(netG.parameters(), lr=0.0001, betas=(0.0, 0.9))\n",
    "optimizerC = optim.Adam(netC.parameters(), lr=0.0001, betas=(0.0, 0.9))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "70nPMd5AXHD2",
    "outputId": "4edc5e0f-fb1e-4c5c-fd6c-ba1309206888"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "[0/25][0/782] Loss_C: 212.3717 Loss_G: -0.0305\n",
      "[0/25][100/782] Loss_C: -1.0466 Loss_G: 1.0596\n",
      "[0/25][200/782] Loss_C: -0.3749 Loss_G: 0.6686\n",
      "[0/25][300/782] Loss_C: -0.2862 Loss_G: 0.5059\n",
      "[0/25][400/782] Loss_C: -0.2509 Loss_G: 0.8586\n",
      "[0/25][500/782] Loss_C: -0.3415 Loss_G: 1.1334\n",
      "[0/25][600/782] Loss_C: -0.1243 Loss_G: 1.1573\n",
      "[0/25][700/782] Loss_C: -0.0881 Loss_G: 1.1234\n",
      "[1/25][0/782] Loss_C: -0.0946 Loss_G: 1.2625\n",
      "[1/25][100/782] Loss_C: -0.1168 Loss_G: 1.3746\n",
      "[1/25][200/782] Loss_C: -0.1947 Loss_G: 1.5372\n",
      "[1/25][300/782] Loss_C: -0.2804 Loss_G: 1.5514\n",
      "[1/25][400/782] Loss_C: -0.2805 Loss_G: 1.6638\n",
      "[1/25][500/782] Loss_C: -0.2814 Loss_G: 1.4599\n",
      "[1/25][600/782] Loss_C: -0.3002 Loss_G: 1.4184\n",
      "[1/25][700/782] Loss_C: -0.2523 Loss_G: 1.2166\n",
      "[2/25][0/782] Loss_C: -0.0579 Loss_G: 1.2048\n",
      "[2/25][100/782] Loss_C: -0.2658 Loss_G: 1.1655\n",
      "[2/25][200/782] Loss_C: -0.2455 Loss_G: 1.1759\n",
      "[2/25][300/782] Loss_C: -0.1925 Loss_G: 1.1379\n",
      "[2/25][400/782] Loss_C: -0.2677 Loss_G: 1.1985\n",
      "[2/25][500/782] Loss_C: -0.2145 Loss_G: 1.1110\n",
      "[2/25][600/782] Loss_C: -0.2404 Loss_G: 1.1907\n",
      "[2/25][700/782] Loss_C: -0.2140 Loss_G: 1.1822\n",
      "[3/25][0/782] Loss_C: -0.0686 Loss_G: 1.0711\n",
      "[3/25][100/782] Loss_C: -0.2244 Loss_G: 1.1436\n",
      "[3/25][200/782] Loss_C: -0.1730 Loss_G: 1.0226\n",
      "[3/25][300/782] Loss_C: -0.1888 Loss_G: 1.1580\n",
      "[3/25][400/782] Loss_C: -0.1631 Loss_G: 1.1732\n",
      "[3/25][500/782] Loss_C: -0.2064 Loss_G: 1.1578\n",
      "[3/25][600/782] Loss_C: -0.1593 Loss_G: 1.2196\n",
      "[3/25][700/782] Loss_C: -0.2000 Loss_G: 1.2425\n",
      "[4/25][0/782] Loss_C: -0.1384 Loss_G: 1.2213\n",
      "[4/25][100/782] Loss_C: -0.1617 Loss_G: 1.2924\n",
      "[4/25][200/782] Loss_C: -0.1819 Loss_G: 1.3629\n",
      "[4/25][300/782] Loss_C: -0.1948 Loss_G: 1.3549\n",
      "[4/25][400/782] Loss_C: -0.1074 Loss_G: 1.3680\n",
      "[4/25][500/782] Loss_C: -0.1204 Loss_G: 1.3493\n",
      "[4/25][600/782] Loss_C: -0.2089 Loss_G: 1.3350\n",
      "[4/25][700/782] Loss_C: -0.1763 Loss_G: 1.3660\n",
      "[5/25][0/782] Loss_C: -0.0681 Loss_G: 1.3596\n",
      "[5/25][100/782] Loss_C: -0.1612 Loss_G: 1.4104\n",
      "[5/25][200/782] Loss_C: -0.1010 Loss_G: 1.2720\n",
      "[5/25][300/782] Loss_C: -0.1543 Loss_G: 1.2943\n",
      "[5/25][400/782] Loss_C: -0.1907 Loss_G: 1.3602\n",
      "[5/25][500/782] Loss_C: -0.1159 Loss_G: 1.2507\n",
      "[5/25][600/782] Loss_C: -0.1656 Loss_G: 1.2434\n",
      "[5/25][700/782] Loss_C: -0.1350 Loss_G: 1.3391\n",
      "[6/25][0/782] Loss_C: -0.1246 Loss_G: 1.2146\n",
      "[6/25][100/782] Loss_C: -0.1728 Loss_G: 1.3339\n",
      "[6/25][200/782] Loss_C: -0.1332 Loss_G: 1.2719\n",
      "[6/25][300/782] Loss_C: -0.2090 Loss_G: 1.3061\n",
      "[6/25][400/782] Loss_C: -0.1344 Loss_G: 1.3052\n",
      "[6/25][500/782] Loss_C: -0.1350 Loss_G: 1.2774\n",
      "[6/25][600/782] Loss_C: -0.1305 Loss_G: 1.2591\n",
      "[6/25][700/782] Loss_C: -0.1175 Loss_G: 1.2078\n",
      "[7/25][0/782] Loss_C: -0.0437 Loss_G: 1.2803\n",
      "[7/25][100/782] Loss_C: -0.1678 Loss_G: 1.3491\n",
      "[7/25][200/782] Loss_C: -0.0277 Loss_G: 1.2371\n",
      "[7/25][300/782] Loss_C: -0.0813 Loss_G: 1.3181\n",
      "[7/25][400/782] Loss_C: -0.1439 Loss_G: 1.2301\n",
      "[7/25][500/782] Loss_C: -0.1568 Loss_G: 1.2720\n",
      "[7/25][600/782] Loss_C: -0.0950 Loss_G: 1.2560\n",
      "[7/25][700/782] Loss_C: -0.1002 Loss_G: 1.2546\n",
      "[8/25][0/782] Loss_C: -0.0401 Loss_G: 1.2376\n",
      "[8/25][100/782] Loss_C: -0.1173 Loss_G: 1.2836\n",
      "[8/25][200/782] Loss_C: -0.1149 Loss_G: 1.3079\n",
      "[8/25][300/782] Loss_C: -0.1332 Loss_G: 1.3073\n",
      "[8/25][400/782] Loss_C: -0.0994 Loss_G: 1.4243\n",
      "[8/25][500/782] Loss_C: -0.1465 Loss_G: 1.3740\n",
      "[8/25][600/782] Loss_C: -0.1313 Loss_G: 1.3934\n",
      "[8/25][700/782] Loss_C: -0.1195 Loss_G: 1.4143\n",
      "[9/25][0/782] Loss_C: -0.1090 Loss_G: 1.3883\n",
      "[9/25][100/782] Loss_C: -0.1044 Loss_G: 1.3609\n",
      "[9/25][200/782] Loss_C: -0.0947 Loss_G: 1.3367\n",
      "[9/25][300/782] Loss_C: -0.1140 Loss_G: 1.4479\n",
      "[9/25][400/782] Loss_C: -0.1108 Loss_G: 1.4068\n",
      "[9/25][500/782] Loss_C: -0.1412 Loss_G: 1.4329\n",
      "[9/25][600/782] Loss_C: -0.1060 Loss_G: 1.3938\n",
      "[9/25][700/782] Loss_C: -0.0337 Loss_G: 1.3899\n",
      "[10/25][0/782] Loss_C: -0.0931 Loss_G: 1.4100\n",
      "[10/25][100/782] Loss_C: -0.1235 Loss_G: 1.5579\n",
      "[10/25][200/782] Loss_C: -0.1086 Loss_G: 1.4100\n",
      "[10/25][300/782] Loss_C: -0.1367 Loss_G: 1.5038\n",
      "[10/25][400/782] Loss_C: -0.1023 Loss_G: 1.6217\n",
      "[10/25][500/782] Loss_C: -0.0802 Loss_G: 1.4947\n",
      "[10/25][600/782] Loss_C: -0.0985 Loss_G: 1.5438\n",
      "[10/25][700/782] Loss_C: -0.0997 Loss_G: 1.5584\n",
      "[11/25][0/782] Loss_C: -0.0972 Loss_G: 1.4893\n",
      "[11/25][100/782] Loss_C: -0.0588 Loss_G: 1.5541\n",
      "[11/25][200/782] Loss_C: -0.0837 Loss_G: 1.5590\n",
      "[11/25][300/782] Loss_C: -0.1192 Loss_G: 1.5438\n",
      "[11/25][400/782] Loss_C: -0.0873 Loss_G: 1.5135\n",
      "[11/25][500/782] Loss_C: -0.1115 Loss_G: 1.6165\n",
      "[11/25][600/782] Loss_C: -0.0866 Loss_G: 1.5467\n",
      "[11/25][700/782] Loss_C: -0.1073 Loss_G: 1.5058\n",
      "[12/25][0/782] Loss_C: -0.0036 Loss_G: 1.5904\n",
      "[12/25][100/782] Loss_C: -0.1390 Loss_G: 1.5742\n",
      "[12/25][200/782] Loss_C: -0.1088 Loss_G: 1.6034\n",
      "[12/25][300/782] Loss_C: -0.1134 Loss_G: 1.5789\n",
      "[12/25][400/782] Loss_C: -0.1040 Loss_G: 1.5716\n",
      "[12/25][500/782] Loss_C: -0.1123 Loss_G: 1.5496\n",
      "[12/25][600/782] Loss_C: -0.0922 Loss_G: 1.5022\n",
      "[12/25][700/782] Loss_C: -0.1011 Loss_G: 1.5831\n",
      "[13/25][0/782] Loss_C: -0.0810 Loss_G: 1.6553\n",
      "[13/25][100/782] Loss_C: -0.0743 Loss_G: 1.5802\n",
      "[13/25][200/782] Loss_C: -0.1465 Loss_G: 1.5349\n",
      "[13/25][300/782] Loss_C: -0.0898 Loss_G: 1.5545\n",
      "[13/25][400/782] Loss_C: -0.1231 Loss_G: 1.5105\n",
      "[13/25][500/782] Loss_C: 0.0244 Loss_G: 1.4979\n",
      "[13/25][600/782] Loss_C: -0.0892 Loss_G: 1.5336\n",
      "[13/25][700/782] Loss_C: 0.0305 Loss_G: 1.5789\n",
      "[14/25][0/782] Loss_C: -0.0282 Loss_G: 1.5077\n",
      "[14/25][100/782] Loss_C: -0.0799 Loss_G: 1.5890\n",
      "[14/25][200/782] Loss_C: -0.0826 Loss_G: 1.5093\n",
      "[14/25][300/782] Loss_C: -0.0550 Loss_G: 1.4128\n",
      "[14/25][400/782] Loss_C: -0.0905 Loss_G: 1.5482\n",
      "[14/25][500/782] Loss_C: -0.0558 Loss_G: 1.4379\n",
      "[14/25][600/782] Loss_C: 0.0504 Loss_G: 1.4315\n",
      "[14/25][700/782] Loss_C: -0.0546 Loss_G: 1.4464\n",
      "[15/25][0/782] Loss_C: 0.0333 Loss_G: 1.5287\n",
      "[15/25][100/782] Loss_C: -0.1185 Loss_G: 1.4442\n",
      "[15/25][200/782] Loss_C: -0.0530 Loss_G: 1.5182\n",
      "[15/25][300/782] Loss_C: -0.1185 Loss_G: 1.5152\n",
      "[15/25][400/782] Loss_C: -0.1026 Loss_G: 1.5092\n",
      "[15/25][500/782] Loss_C: -0.0706 Loss_G: 1.4647\n",
      "[15/25][600/782] Loss_C: -0.0881 Loss_G: 1.4541\n",
      "[15/25][700/782] Loss_C: -0.0739 Loss_G: 1.5059\n",
      "[16/25][0/782] Loss_C: -0.0163 Loss_G: 1.3601\n",
      "[16/25][100/782] Loss_C: -0.0864 Loss_G: 1.4665\n",
      "[16/25][200/782] Loss_C: -0.0079 Loss_G: 1.3938\n",
      "[16/25][300/782] Loss_C: -0.0721 Loss_G: 1.4755\n",
      "[16/25][400/782] Loss_C: -0.1023 Loss_G: 1.5279\n",
      "[16/25][500/782] Loss_C: -0.0853 Loss_G: 1.4937\n",
      "[16/25][600/782] Loss_C: -0.0858 Loss_G: 1.4961\n",
      "[16/25][700/782] Loss_C: -0.1216 Loss_G: 1.4439\n",
      "[17/25][0/782] Loss_C: 0.0095 Loss_G: 1.3827\n",
      "[17/25][100/782] Loss_C: -0.0981 Loss_G: 1.3871\n",
      "[17/25][200/782] Loss_C: -0.0770 Loss_G: 1.4067\n",
      "[17/25][300/782] Loss_C: -0.0910 Loss_G: 1.4474\n",
      "[17/25][400/782] Loss_C: -0.0763 Loss_G: 1.4096\n",
      "[17/25][500/782] Loss_C: -0.0708 Loss_G: 1.3989\n",
      "[17/25][600/782] Loss_C: -0.0428 Loss_G: 1.3994\n",
      "[17/25][700/782] Loss_C: -0.0488 Loss_G: 1.4096\n",
      "[18/25][0/782] Loss_C: 0.1526 Loss_G: 1.3980\n",
      "[18/25][100/782] Loss_C: -0.0578 Loss_G: 1.3962\n",
      "[18/25][200/782] Loss_C: -0.0686 Loss_G: 1.4015\n",
      "[18/25][300/782] Loss_C: -0.0700 Loss_G: 1.4106\n",
      "[18/25][400/782] Loss_C: -0.0880 Loss_G: 1.4053\n",
      "[18/25][500/782] Loss_C: -0.0640 Loss_G: 1.3939\n",
      "[18/25][600/782] Loss_C: -0.1151 Loss_G: 1.2888\n",
      "[18/25][700/782] Loss_C: -0.0777 Loss_G: 1.3253\n",
      "[19/25][0/782] Loss_C: -0.0254 Loss_G: 1.3262\n",
      "[19/25][100/782] Loss_C: -0.1040 Loss_G: 1.3420\n",
      "[19/25][200/782] Loss_C: -0.0430 Loss_G: 1.3356\n",
      "[19/25][300/782] Loss_C: -0.0962 Loss_G: 1.4034\n",
      "[19/25][400/782] Loss_C: -0.0715 Loss_G: 1.3605\n",
      "[19/25][500/782] Loss_C: -0.0778 Loss_G: 1.4329\n",
      "[19/25][600/782] Loss_C: -0.0377 Loss_G: 1.3047\n",
      "[19/25][700/782] Loss_C: -0.0422 Loss_G: 1.3057\n",
      "[20/25][0/782] Loss_C: -0.0486 Loss_G: 1.3433\n",
      "[20/25][100/782] Loss_C: 0.1914 Loss_G: 1.2625\n",
      "[20/25][200/782] Loss_C: -0.0862 Loss_G: 1.3100\n",
      "[20/25][300/782] Loss_C: -0.0799 Loss_G: 1.4171\n",
      "[20/25][400/782] Loss_C: -0.0875 Loss_G: 1.3335\n",
      "[20/25][500/782] Loss_C: -0.0413 Loss_G: 1.3222\n",
      "[20/25][600/782] Loss_C: -0.0778 Loss_G: 1.3580\n",
      "[20/25][700/782] Loss_C: -0.0470 Loss_G: 1.3440\n",
      "[21/25][0/782] Loss_C: -0.0195 Loss_G: 1.2498\n",
      "[21/25][100/782] Loss_C: 0.0160 Loss_G: 1.3054\n",
      "[21/25][200/782] Loss_C: -0.0976 Loss_G: 1.3228\n",
      "[21/25][300/782] Loss_C: -0.0537 Loss_G: 1.2912\n",
      "[21/25][400/782] Loss_C: -0.1379 Loss_G: 1.2469\n",
      "[21/25][500/782] Loss_C: -0.1074 Loss_G: 1.3215\n",
      "[21/25][600/782] Loss_C: -0.0019 Loss_G: 1.3182\n",
      "[21/25][700/782] Loss_C: -0.0855 Loss_G: 1.3462\n",
      "[22/25][0/782] Loss_C: -0.0554 Loss_G: 1.3541\n",
      "[22/25][100/782] Loss_C: -0.0571 Loss_G: 1.2348\n",
      "[22/25][200/782] Loss_C: -0.0483 Loss_G: 1.2524\n",
      "[22/25][300/782] Loss_C: -0.0376 Loss_G: 1.4053\n",
      "[22/25][400/782] Loss_C: -0.0222 Loss_G: 1.3584\n",
      "[22/25][500/782] Loss_C: -0.1256 Loss_G: 1.2161\n",
      "[22/25][600/782] Loss_C: -0.0298 Loss_G: 1.2871\n",
      "[22/25][700/782] Loss_C: -0.0551 Loss_G: 1.2477\n",
      "[23/25][0/782] Loss_C: -0.0182 Loss_G: 1.2061\n",
      "[23/25][100/782] Loss_C: -0.0476 Loss_G: 1.2815\n",
      "[23/25][200/782] Loss_C: 0.0247 Loss_G: 1.2492\n",
      "[23/25][300/782] Loss_C: -0.0885 Loss_G: 1.2505\n",
      "[23/25][400/782] Loss_C: -0.0928 Loss_G: 1.2329\n",
      "[23/25][500/782] Loss_C: -0.0640 Loss_G: 1.2593\n",
      "[23/25][600/782] Loss_C: -0.0647 Loss_G: 1.2642\n",
      "[23/25][700/782] Loss_C: -0.0536 Loss_G: 1.1412\n",
      "[24/25][0/782] Loss_C: -0.0063 Loss_G: 1.2246\n",
      "[24/25][100/782] Loss_C: -0.0822 Loss_G: 1.1775\n",
      "[24/25][200/782] Loss_C: -0.0509 Loss_G: 1.2022\n",
      "[24/25][300/782] Loss_C: -0.0418 Loss_G: 1.2209\n",
      "[24/25][400/782] Loss_C: -0.0918 Loss_G: 1.2083\n",
      "[24/25][500/782] Loss_C: -0.0235 Loss_G: 1.2136\n",
      "[24/25][600/782] Loss_C: -0.0778 Loss_G: 1.2119\n",
      "[24/25][700/782] Loss_C: -0.0474 Loss_G: 1.1982\n"
     ]
    }
   ],
   "source": [
    "n_critic = 5  # Update critic 5 times per generator update\n",
    "lambda_gp = 10  # Gradient penalty weight\n",
    "# Khởi tạo dataloader và số lượng kênh\n",
    "dataloader, nc = load_data('cifar10')\n",
    "\n",
    "for epoch in range(25):\n",
    "    for i, data in enumerate(dataloader):\n",
    "        # Train Critic\n",
    "        netC.zero_grad()\n",
    "        real_data = data[0].to(device)\n",
    "\n",
    "        batch_size = real_data.size(0)\n",
    "        noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "        fake_data = netG(noise).detach()\n",
    "\n",
    "        real_score = netC(real_data).mean()\n",
    "        fake_score = netC(fake_data).mean()\n",
    "\n",
    "        # Compute gradient penalty\n",
    "        gp = gradient_penalty(netC, real_data, fake_data, device)\n",
    "\n",
    "        # Critic loss\n",
    "        lossC = fake_score - real_score + lambda_gp * gp\n",
    "        lossC.backward()\n",
    "        optimizerC.step()\n",
    "\n",
    "        # Update Generator every n_critic iterations\n",
    "        if i % n_critic == 0:\n",
    "            netG.zero_grad()\n",
    "            noise = torch.randn(batch_size, nz, 1, 1, device=device)\n",
    "            fake_data = netG(noise)\n",
    "            lossG = -netC(fake_data).mean()\n",
    "            lossG.backward()\n",
    "            optimizerG.step()\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            print(f'[{epoch}/{25}][{i}/{len(dataloader)}] '\n",
    "                  f'Loss_C: {lossC.item():.4f} Loss_G: {lossG.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PLRe4aq3XJCm"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
